\chapter{Analysis and Results}


\section{2022 vdM scan program}

In 2022, the main vdM scan program for the CMS experiment was conducted during LHC fills 8379 and 8381 on 10 and 11 November. During fill 8381, a total of 144 bunch pairs were collided head-on , with a proton-proton collision energy $\sqrt{s}$ of 13.6 TeV. 
During this fill, various types of scan pairs with different characteristics were conducted. These scan pairs are described below. Each scan pair consisted of two scans in the transverse planes $x$ and $y$. In addition to these scans, other types of scans were also conducted, which are not included in this analysis.\\ 

The calibration program included four scan pairs, vdM1,vdM2, vdM3 and vdM4, these are considered the standard vdM scans in the program, used specifically to compute $\sigma_{vis}$. In this type of scan the two beams are separated by $6\sigma_{b} \thickapprox 578\mu m$, where $\sigma_{b}$ represents the transverse bunch size. The beams are then scanned across each other in a sequence of 25 steps, each lasting 30 seconds with a step size of $0.5\sigma_{b} \thickapprox 48 \mu m$.\\

The calibration program also included two beam-imaging scan pairs, BI1 and BI2 . These scans were developed for studies on beam shapes, estimating the uncertainty caused by the not strictly valid assumption seen in previous sections that the bunch proton density function is factorizable into independent $x$ and $y$ densitys, this correction is called  ($x$-$y$ nonfactorization). BI acans also are used to compute of $\sigma_{vis}$, independent of the standard vdM's scans. During the BI scans, one beam is kept fixed at its nominal head-on position, while the other beam is separated and scanned in 19 steps, each step lasts 46 seconds and covers a range from $-4.5\sigma_{b}$ to $+4.5\sigma_{b} \thickapprox 433 \mu m$.\\

Two length scale LS scans were conducted, one during fill 8381 and another during fill 8379. The LS scan is a special scan designed to apply a correction to the results of  $\sigma_{vis}$ obtained from the vdM scan program, this correction will be described further in this section. During this scan, the beams are separated by a constant amount of $\sqrt{2} \sigma_{b} \thickapprox 106 \mu m$, and they are moved coherently forward and backward in five steps each, all in the same transverse direction.\\

Finally two super separation periods SS1 and SS2 were performed for the background estimation, here the two beams were sepated at a distance of $6\sigma_{b}$ during 5 minutes, at this distance the overlap of the beams is minimal and the contribution of collision is negligible.
%23:31-23:36 11:47-11:52

The scan program during fill 8381 is summarized in Figure \ref{scan_prog} where all the scans mentioned and described above are labeled.

\begin{center}
  \begin{figure}[h]	
    \centering
    \includegraphics[scale=.35]{Chapter4/2022Scanprorgam.png}
    \caption[2022 vdM scan program]{Nominal horizontal and vertical positions of the proton beams during LHC fill 8381, showing the four standard vdM scans, two BI scans, one LS scan and the two Super Separation Periods (SS)}
    \label{scan_prog}
  \end{figure}
\end{center}

\section{Data aquisition and processing}
\label{data}
In order to achieve a high statistical precision in data for each colliding bunch pairs the system trigger  bandwidth  limitations do not allow the use of all of them, beacause this, CMS implemented a zero-bias triggers only on five Bunch crossing id (BCID): 282, 822, 2944, 3123, and 3302. This data is saved randomly with the highest possible rate of 27.23 kHz using the High Level Trigger (HLT) through the CMS data acquisition (DAQ) system. Due to the weight of the events, 16 different streams are required to save the full data. Here the datasets are in their RAW format, which contains information per event (bunch crossing) and the amount of charge deposited in each pixel.\\ 
During the vdM2 scan pair, the central CMS data taking was not operational, so no data was recorded for PCC, and this scan pair was skipped from the full calibration.\\

The data is then reprocessed by the CMS collaboration to enable a cluster reconstruction, resulting in the datasets (ALCARECO version) that contain a module collection and their number of clusters per event , the data here is in the CMSSW format. These datasets will be used as the starting point for our own data reprocessing and analysis.\\

The ALCARECO samples are processed using CMSSW software (written in Python and C++) to extract the clusters per module. Due to the large size of the datasets, all the datasets were sent for reprocessing through The Worldwide LHC Computing Grid (WLCG), wish consists of a grid-based computer network infrastructure,designed by CERN to handle the big volume of data produced by LHC experiments. In this  process, we obtained the required cluster counts, which was saved in a dataset with a ROOT format. This format uses TTrees and TBranches, that are containers to maintain the information in a hierarchical and organized manner. 
The information is stored in one TTree that contains the information per event distributed in several TBranches, such as the number of pixel clusters, the BCID, the module identifiers, number of run, the number of lumisection (where 1 LS = 23 s), the number of luminible LN (where 1 LN is equivalent to 0.32 s) and time where that event took place (timestamp begin).\\

In order to maintain full statistical precision despite the limitations of the vdM framework (vdMFW), which was utilized for the final fit of our data, the rates were stored  each 1.32 s (NB4) time that corresponding to 4 LN, this was done making  an average  between clusters and the number of events that fall within this time span. During this process, the module veto list mentioned in the previous section is introduced and the modules detected in these list were removed. In this phase, the information is merged into a single file using a hierarchical data format (hd5) that organizes the data into tables, this final format is the required by the vdMFW. All the  process is carried out using lxbatch, a cluster of computers located within CERN.\\

Finally the hd5 file containing the rates collected during the vdM scans  is analyzed using the vdMFW, which is written in Python and uses the ROOT data analysis package through the PyRoot library. The vdMFW  extracts the necessary information using its analysis tools. During the process, several corrections are applied to the rates which will be described later. In the processing the vdMFW generates the final information and plots, which contain the normalized rates ($R/N_{1}N_{2}$) and beam position.  A mathematical function is then used to fit the points to extract $\Sigma_{x,y}$ and peak values to compute $\sigma_{\text{vis}}$ for all the bunches in the scan.

\section{Background estimation}
\label{bkg}
To perform accurate fitting of the data, it is necessary to take into account the following background contributions to the PCC rates. These contributions are estimated independently during the Super Separation (SS) periods discussed in the previous section. Three primary sources of beam-induced background should be considered:

\begin{itemize}

\item Beam Gas Elastic (BGE). The elastic beam gas contribution are all coherent and quasi-elastic, nuclear elastic and coulomb scattering for multi-turn beam-gas interactions around the ring. Typically the interaction of the primary beam proton and the takes place at the tertiary TCT collimator jaw \cite{bkg_source}.

\item Beam Gas Inelastic (BGI). These are all inelastic interactions of primary beam protons with rest gas in the beam pipe. The interaction rate is dominated by the vacuum quality in the various beam line elements upstream CMS. Therefore the origin of this contribution is distributed all along the long straight section \cite{bkg_source}.

\item Beam Halo (BH). This component of the machine induced background is caused by the inefficiency of the main collimation system. Protons can escape from one collimator and being intercepted by another collimator \cite{bkg_source}.
\end{itemize}

In order to illustrate the two SS periods for this analisys, Figure \ref{ssp_wide_bx282} displays the PCC per NB4 versus time for BCID 282. The lower regions of the plot correspond to a time window of 5 minutes during which the beams were separated by a distance of $6\sigma_{b}$.\\

\begin{center}
\begin{figure}[h!]
\centering
\includegraphics[width=.49\textwidth]{Chapter4/BX_282_Rates_SS1.png}
\includegraphics[width=.49\textwidth]{Chapter4/BX_282_Rates_SS2.png}
\caption[Super Separation periods for BCID 282]{PCC per NB4 versus time for BCID 282. The lower regions corresponds to the super separation period I (left) and II (right).}
\label{ssp_wide_bx282}
\end{figure}
\end{center}

To estimate the background value in each BCID, the mean value of the PCC per NB4 rates distribution is computed. The mean and error per super separation (SS) period are obtained by plotting the PCC per NB4 as a function of time and then taking the $y$ projection. The error is calculated using the formula $AvgErr = \sqrt{SEM_1^2 + SEM_2^2 + \cdots + SEM_N}/N$. To illustrate this process for each BCID, the distribution or profile of PCC per NB4 for each SS period is plotted. Figure \ref{ss1_hist_282} displays the PCC per NB4 distribution for SS period I, specifically for BCID 282, but the same plot is created for every BCID.\\

\begin{center}
  \begin{figure}[h!]
    \centering
    \includegraphics[scale=.30]{Chapter4/ss1_histo_bx282.png}
    \caption[PCC per NB4 profile for BCID 282 from SS period I]{ PCC per NB4 profile for BCID 282 from SS period I.} 
    \label{ss1_hist_282}
  \end{figure}
\end{center}


\begin{table}[h!]
  \begin{center}
    \caption[Background values for the two super separation periods]{Mean and standard error on mean (SEM) for the PCC background estimated with the SS1 and SS2 data, separately for all five BCIDs and averaged.}
    \label{ss_per_bx}
    \begin{tabular}{|c | c | c | }
      \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{SS period I}} & \multicolumn{1}{c}{}  \\
      \hline
 \textbf{BCID}   & \textbf{Mean}   &  \textbf{SEM}\\
     \hline %\midrule[1.1pt]
      282 & 0.7712 & 0.0054\\
      \hline
      822 & 0.7744 & 0.0055\\ 
      \hline
      2944 & 0.7695 & 0.0057\\ 
      \hline
      3123 & 0.7469 & 0.0049\\ 
      \hline
      3302 & 0.7708 & 0.0049\\ 
      \hline
      \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
      \multicolumn{1}{c}{$\text{SSI}_{\text{Avg}}=$} & \multicolumn{1}{l}{$0.7665 \pm 0.0023$} & \multicolumn{1}{c}{}\\
%      \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}
    \end{tabular}
    \hspace{0.5cm}
    \begin{tabular}{|c | c | c | }
      \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{SS period II}} & \multicolumn{1}{c}{ }  \\
      \hline
 \textbf{BCID}   & \textbf{Mean}   &  \textbf{SEM}\\
     \hline %\midrule[1.1pt]
      282 & 0.7192 & 0.0048\\
      \hline
      822 & 0.7257 & 0.0057\\ 
      \hline
      2944 & 0.7292 & 0.0058\\ 
      \hline
      3123 & 0.7028 & 0.0052\\ 
      \hline
      3302 & 0.7208 & 0.0053\\ 
      \hline
      \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
      \multicolumn{1}{c}{$\text{SSII}_{\text{Avg}}=$} & \multicolumn{1}{l}{$ 0.7195 \pm 0.0024$} & \multicolumn{1}{c}{}
    \end{tabular}   
  \end{center}
\end{table}

Table  \ref{ss_per_bx} shows the results for five different BCIDs, using the reprocessed PCC data for Fill 8381. This analysis reveals notable inconsistencies in the background values obtained from the two Super Separation (SS) periods. As a result, the quality of the fit was affected, and thus, this data was excluded from subsequent analyses. Further details on this matter will be discussed in a later section.

\section{Beam Corrections}

There are several systematic effects that affect the measurement of beam overlap width and therefore the extraction of $\sigma_{vis}$ from the vdM scan procedure. These effects are measured and, where applicable, corrected as described below. A systematic uncertainty is assigned to the resulting measured cross section $\sigma_{vis}$, and the following corrections are applied in the vdM Framework:
 
\begin{enumerate}

\item Ghost and Satellite. Corrects the presence of spurious charges, which affect bunch currents. Satellite charges refer to additional charges outside the actual colliding bunch, while ghost charges refer to charge not in any nominally filled bunch slot, these results in corrections of up to 0.5\% to $\sigma_{vis}$. The overall uncertaintys estimate the bunch current measurement  to be 0.4\%.

\item Orbit drift. This correction accounts for potential movement of the LHC orbit during the vdM scans. Time-dependent changes of the transverse beam positions for fixed machine parameters can alter the beam separation, this correction is composed of two independent corrections:  

Orbit drift separation "ODS", correction that aims to correct for the orbit drift in the scanning direction and only affects beam separation and  Orbit drift rate "ODR", correction that aims to correct for the orbit drift in the direction orthogonal to the scanning direction and only affects the luminometer rate. 
%The derived correction assumes that the beam overlap has a single Gaussian shape. The correction reads the $\sigma_{vis}$ in the orthogonal direction from the previous correction. 

Applying this correction improves the agreement between the $\sigma_{vis}$ values derived with different scan pairs and changes the average $\sigma_{vis}$ by about 0.3\%, considering the full size of the correction is as uncertainty.

\item Beam Beam corrections.  The electromagnetic interaction between the two colliding proton bunches leads to two effects in beam-separation scans:

Beam Beam deflection (BB). Corrects Beam Beam deflection that happens during bunch crossings at the collision point. Accounts for the electrical repulsion of the beams, which increases the lateral separation. The deflection is calculated and added to the nominal separation.

Dynamic Beta. The so-called dynamic $\beta^{*}$  effect, which accounts for any changes in the proton density distributions of the bunches due to the single-particle interactions. As a result, the non-linear change during separation steps in transverse bunch profiles is observed, and can described by the effective change of the $\beta^{*}$  value.

The corrections are calculated for each proton bunch pair individually, and the combined effect of the two corrections is an increase of $\sigma_{vis}$ by 1.0\%, with an uncertainty of 0.5\%.

\item Length Scale.  It applies a linear scaling to the beam separation to convert it from the "CMS scale" to the actual "physics scale". This is  calibrated by analyzing pp collision vertices reconstruced by the CMS tracker using data from the five length scale scan pairs performed in fills 8178, 8379, and 8381.This correction reduces $\sigma_{vis}$ by 1.0\% and results in an uncertainty of 0.12\%.

\end{enumerate}

\section{Fit Model  Selection}

As we have seen in previous sections, the points generated for each step of the scan need to be fitted using a mathematical function. This allows us to extract the parameters we need, such as $\Sigma_{x,y}$ and peak values for all the bunches in the scan, in order to finally compute $\sigma_{\text{vis}}$. Within vdMFW, there are several pre-defined functions that use a combination of Gaussian functions with different adjustable parameters to achieve a better fit. These functions include Double Gaussian (DG), Single Gaussian (SG), Poly Gaussians (PG), among others. All of the above functions exist in versions with an additive constant to the entire function, typically used to fit to the background levels.\\

For this analysis, various fit functions were tested, and the one with the best quality of fit was selected. The quality of the fit is primarily based on the convergence of the function with the points being fitted for each BCID. The convergence is assessed based on the covariance matrix, which measure the linear relationship between each pair of elements or variables. A  covariance status equal to 3 means a good convergence of the function.\\ 

The second most important parameter for measuring the quality of the fit is having a good  Chi-square $\chi^2$.  The  Chi-square goodness of fit test is a statistical hypothesis test that helps to determine whether a variable is likely to come from a specified distribution or not. This test provides a way to assess whether the data values have a good enough fit to our model, or if the model is questionable. In more general terms, this test compares observed values with expected or theoretical values according to the hypothesis. The goal is to minimize the calculation of the $\chi^2$ as defined in equation \ref{x^2}, from here is  inferred that the lower the value of $\chi^2$, the better the fit is considered to be.

\begin{equation}
\chi^{2}(\theta)=\sum_{i=1}^{N} \frac{(y_{i}-f(x_{i},\theta))^{2}}{\sigma_{i}^{2}}
\label{x^2}
\end{equation}

where the observed values $y_{1}$,···, $y_{N}$ are measured with errors $\sigma_{1}$,···,$\sigma_{N}$" at the values of $x$ given without error by $x_{1}$,..., $x_{N}$. The theoretical  value is given by the function $f(x_{i},\theta)$. The value of $\theta$ is adjusted to minimize the value of $\chi^2$ \cite{Statistical_Data_Analysis}.

After testing several mathematical fit models, those requiring background subtraction from SS periods analysis were ultimately discarded due to the unacceptably high chi-square values obtained, this was due to the inconsistencies mentioned the section \ref{bkg}. Therefore, it was decided to use functions that include a constant to model the background in the fit. The chosen model with the best fit quality and after applying all the corrections mentioned earlier, was a double Gaussian function plus a constant:
%https://gitlab.cern.ch/bril/VdMFramework/-/blob/main/src/plotting/capsigma_over_time.py#L90
%\begin{equation}
%DG+Const= C+P \cdot \Biggl[ F \cdot \exp \Biggl( \frac{-(x-\bar{x})^{2}}{2 \sigma_{1}^{2} } \Biggr) + (1-F) \cdot \exp \Biggl( \frac{-(x-\bar{x})^{2}}{2 \sigma_{2}^{2} } \Biggr) \Biggr] 
%\end{equation}
\begin{equation}
f(x)= C+P \cdot \Biggl[ F \cdot \exp \Biggl( \frac{-(x-\bar{x})^{2}}{2(\frac{\Sigma\cdot R}{F \cdot R+1-F} )^{2} } \Biggr) + (1-F) \cdot \exp \Biggl( \frac{-(x-\bar{x})^{2}}{2 (\frac{\Sigma}{F \cdot R+1-F} )^{2} } \Biggr) \Biggr] 
\end{equation}
%[5] + [2]*([3]*exp(-(x-[4])**2/(2*([0]*[1]/([3]*[1]+1-[3]))**2))
    %    + (1-[3])*exp(-(x-[4])**2/(2*([0]/([3]*[1]+1-[3]))**2)) )
%("#Sigma","#sigma_{1}/#sigma_{2}","peak","Frac","Mean","Const")
%[0] -> [0]*[1]/([3]*[1]+1-[3])
 %[1] -> [0]/([3]*[1]+1-[3])"""
\begin{itemize}
\item $C$ is the constant value.
\item $P$ the $peak$ rate.
\item $F$ is the fraction of the peak rate attributed to the first Gaussian in the sum.
\item $x$ is the beam position.
\item $\bar{x}$ is the mean.
\item $\Sigma$ is the overllap between the beams.
\item $R$ is the ratio between the standard deviations of the  Gaussians (first over the second).
\end{itemize}

Figure \ref{vdM1_282_XYscan} shows the fit graphs of the first vdM scan pair for BCID 282 where  all the corrections described in the previous section have been applied and where  rates are normalized by the beam currents ($N_{1}N_{2}$), which is according of Eq. \ref{sigmavis_eq}.

\begin{center}
\begin{figure}[h!]
\centering
\includegraphics[width=.45\textwidth]{Chapter4/xscan.png}
\includegraphics[width=.45\textwidth]{Chapter4/yscan.png}\\
\caption[Fit for the vdM1 BCIDs 282 (x and y directions)]{Normalized rates and the resulting fitted curves with  fit model as a function of the beam separation ($\Delta$) for BCID 282 for $x$ (left) and $y$ (right) scan for the first vdM scan.}
\label{vdM1_282_XYscan}
\end{figure}
\end{center}
For the remaining BCID's, the fits for both vdM and BI scans exhibit a very similar behavior, and in all cases, the fits have converged successfully. The evaluation of the goodness of fit is shown in Fig. \ref{chi2/ndof} where displays a plot of the $\chi^{2}/ndof$ for all the five scan pairs, which has a mean value of 1.88.

\begin{center}
  \begin{figure}[h!]
    \centering
    \includegraphics[scale=.030]{Chapter4/DGConst_chi2.png}
    \caption[$\chi^{2}$/ndof per BCID for all scan pairs]{ chi2/ndof for all the scan pairs.  where the red dots represent the $x$-scans, and the blue dots represent the $y$-scans. It is important to note that in each scan, the BCIDs follow the same order, which is 282, 822, 2944, 3123, and 3302.} 
    \label{chi2/ndof}
  \end{figure}
\end{center}

\section{Results}

To compute $\sigma_{vis}$, the $\Sigma_{x,y}$ and $peak_{x,y}$ values have been extracted, the behavior of these values ​​for all scans pair are shown in the  Fig. \ref{capsigma_peak} (left) for $\Sigma_{x,y}$ and Fig. \ref{capsigma_peak} (right) for the $peak_{x,y}$ values , where those belonging to the scan in $x$ are presented in red and those belonging to the scan in $y$ are shown in blue.\\

The values of $\Sigma_{x,y}$ wish provide a direct measurement of the beam shape show a decrease in both the $x$ and $y$ directions during the fill time, this is due to a phenomenon called "damping", which occurs due to the interaction between the particles in the beam and the electromagnetic fields in the LHC ring, these fields exert a force on the particles, causing them to lose energy and amplitude, which causes the beam to gradually focus with time. On the other hand the values ​​for the $peak$ are then a direct consequence of this effect, since a more focused beam means more collisions and thus a higher rate.\\

The figure also displays a discontinuity in values between the BI2 and vdM3 scans, which can be attributed to the exclusion of the vdM2 scan for the reasons mentioned in the section \ref{data} . 

\begin{center}
  \begin{figure}[h!]
    \centering
    \includegraphics[width=.49\textwidth]{Chapter4/DGConst_CapSigma.png}
    \includegraphics[width=.48\textwidth]{Chapter4/DGConst_peak.png}
    \caption[$\Sigma_{x,y}$ and peak values per BCID for all scan pairs]{$\Sigma_{x,y}$ (left) and peak values (right) extracted from the fit to compute $\sigma_{vis}$, where the red dots represent the $x$-scans, and the blue dots represent the $y$-scans, note that in each scan, the BCIDs follow the same order, which is 282, 822, 2944, 3123, and 3302.} 
    \label{capsigma_peak}
  \end{figure}
\end{center}

The $\sigma_{vis}$ for all BCIDs (vdM and BI scans) is then computed from the fit results $\Sigma_{x,y}$ and $peak_{x,y}$ shown above, this is done using the Eq. \ref{sigmavis_eq}. The value of $R(0,0)$ is obtained is obtained by taking the average of the peaks in the $x$ and $y$ scans, which has already been normalized. As a result, Equation \ref{sigmavis_eq} is rewritten as:

\begin{equation}
 \sigma_{vis}= 2\pi \Sigma_{x} \Sigma_{y} \Biggl( \frac{peak_{x}+peak_{y}}{2} \Biggr)
 \end{equation}

Figure \ref{sigmavis_perbcid} shows the  value of $\sigma_{vis}$ per BCID for all the scans where the error on $\sigma_{vis}$ is assigned as: 

\begin{equation}
\sigma_{vis\text{Err}}= 2 \pi \sqrt{ (\Sigma_{y} \cdot P \cdot \Sigma_{x\text{Err}})^{2} + (\Sigma_{x} \cdot P \cdot \Sigma_{y \text{Err}})^{2} + (\Sigma_{x} \Sigma_{y} P_{\text{Err}})^{2} }
\end{equation}

\begin{center}
  \begin{figure}[h!]
    \centering
    \includegraphics[scale=.22]{Chapter4/DGConst_xsec.png}
    \caption[$\sigma_{vis}$ per BCID for all scans]{ $\sigma_{vis}$ per BCID for all the scans,  in each scan, the BCIDs follow the same order, which is 282, 822, 2944, 3123, and 3302.}
    \label{sigmavis_perbcid}
  \end{figure}
\end{center}

To obtain the final value of sigma visible , we first perform a weighted average using 5 BCIDs for each scan as shown in Figure \ref{sigmavis_perscan} with blue dots. Subsequently, we perform another weighted average among the results of these 5 scans, this final value is represented by the red line in Figure \ref{sigmavis_perscan}. In both calcuation, it is assumed that there is no correlation between the BCIDs and the scans. Therefore, the computation of the weighted average and the total error  follows Equations \ref{average} and \ref{error} respectively, where the weight is assigned as $w_{i} = 1/\sigma_{visErr}^{2}$. 


\begin{eqnarray}
\sigma_{vis}^{Avg}=\Biggl(\displaystyle\sum_{i} w_{i}\sigma_{vis}^{i} \Biggr)\Biggl( \displaystyle\sum_{i} w_{i} \Biggr)^{-1}
\label{average}
\end{eqnarray}

\begin{eqnarray}
\sigma_{vis,Err}^{Avg}=\frac{1}{\sqrt{ \displaystyle\sum_{i} w_{i}}} 
\label{error}
\end{eqnarray}

The final result obtained is:
\begin{equation}
\sigma_{vis}=4163.3 \pm 3.3 \text{ (stat)  mb}
\end{equation}

\begin{center}
  \begin{figure}[ht]
    \centering
    \includegraphics[scale=0.45]{Chapter4/xsec_perscan_v2.png}
    \caption[$\sigma_{vis}$ average per scan and final result]{ $\sigma_{vis}$  per scan.} 
    \label{sigmavis_perscan}
  \end{figure}
\end{center}

As depicted in Figure \ref{sigmavis_perscan}, the scans demonstrate an RMS of $12.8 \text{ mb}$ for the $\sigma_{vis}$ values, which is larger than the statistical uncertainty. Notably, vdM1 and vdM3 exhibit the largest variations, deviating by approximately 2 standard deviations from the mean. These discrepancies can be attributed to systematic effects in the detector during the scans. Given that other luminometers display a similar trend, this could be due to a common systematic effect such as beam position or beam current $(N_{1}*N_{2})$. A study of the sources of systematic uncertainty will be performed in the future. In this work, we assign the RMS value as an estimate of the systematic uncertainty.

